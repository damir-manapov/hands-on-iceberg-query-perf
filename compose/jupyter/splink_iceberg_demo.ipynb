{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splink with Spark Backend - Iceberg Tables Demo\n",
        "\n",
        "This notebook demonstrates how to use Splink with Spark backend to read data from Iceberg tables stored in MinIO via Nessie catalog.\n",
        "\n",
        "## Prerequisites\n",
        "- All required packages are pre-installed in the container\n",
        "- Spark session is configured for Iceberg and MinIO integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing basic imports...\n",
            "âœ… os imported\n",
            "âœ… SparkSession imported\n",
            "âœ… Basic imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Test basic imports first\n",
        "print(\"Testing basic imports...\")\n",
        "\n",
        "try:\n",
        "    import os\n",
        "    print(\"âœ… os imported\")\n",
        "    \n",
        "    from pyspark.sql import SparkSession\n",
        "    print(\"âœ… SparkSession imported\")\n",
        "    \n",
        "    print(\"âœ… Basic imports successful!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Splink imports...\n",
            "âœ… Linker imported\n",
            "âœ… SparkAPI imported\n",
            "âœ… Comparison functions imported\n",
            "âœ… All Splink imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Test Splink imports\n",
        "print(\"Testing Splink imports...\")\n",
        "\n",
        "try:\n",
        "    from splink import Linker\n",
        "    print(\"âœ… Linker imported\")\n",
        "    \n",
        "    from splink.backends.spark import SparkAPI\n",
        "    print(\"âœ… SparkAPI imported\")\n",
        "    \n",
        "    from splink.comparison_library import ExactMatch, LevenshteinAtThresholds\n",
        "    print(\"âœ… Comparison functions imported\")\n",
        "    \n",
        "    print(\"âœ… All Splink imports successful!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Splink import error: {e}\")\n",
        "    print(\"Installing Splink...\")\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', 'splink[spark]'], check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… AWS environment variables set for MinIO access\n"
          ]
        }
      ],
      "source": [
        "# Set AWS environment variables for MinIO access\n",
        "import os\n",
        "\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio12345'\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
        "os.environ['AWS_ENDPOINT_URL'] = 'http://minio:9000'\n",
        "os.environ['AWS_ENDPOINT_URL_S3'] = 'http://minio:9000'\n",
        "\n",
        "print(\"âœ… AWS environment variables set for MinIO access\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Spark session...\n",
            "âœ… Spark session initialized successfully!\n",
            "Spark version: 3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark session with Iceberg support\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "print(\"Initializing Spark session...\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SplinkIcebergDemo\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.type\", \"nessie\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.ref\", \"main\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://warehouse/\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio12345\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"âœ… Spark session initialized successfully!\")\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available catalogs:\n",
            "+-------------+\n",
            "|      catalog|\n",
            "+-------------+\n",
            "|spark_catalog|\n",
            "+-------------+\n",
            "\n",
            "\n",
            "Available databases in nessie catalog:\n"
          ]
        },
        {
          "ename": "UnsupportedOperationException",
          "evalue": "Unknown catalog type: nessie",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOW CATALOGS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvailable databases in nessie catalog:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW DATABASES IN nessie\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvailable tables in nessie catalog:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOW TABLES IN nessie\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mUnsupportedOperationException\u001b[0m: Unknown catalog type: nessie"
          ]
        }
      ],
      "source": [
        "# List available catalogs and tables\n",
        "print(\"Available catalogs:\")\n",
        "spark.sql(\"SHOW CATALOGS\").show()\n",
        "\n",
        "print(\"\\nAvailable databases in nessie catalog:\")\n",
        "spark.sql(\"SHOW DATABASES IN nessie\").show()\n",
        "\n",
        "print(\"\\nAvailable tables in nessie catalog:\")\n",
        "spark.sql(\"SHOW TABLES IN nessie\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data for testing Splink\n",
        "from pyspark.sql import Row\n",
        "\n",
        "print(\"Creating sample data for Splink testing...\")\n",
        "\n",
        "# Sample data for testing Splink\n",
        "sample_data = [\n",
        "    Row(id=1, first_name=\"John\", last_name=\"Smith\", email=\"john.smith@email.com\", phone=\"555-1234\"),\n",
        "    Row(id=2, first_name=\"Jon\", last_name=\"Smith\", email=\"jon.smith@email.com\", phone=\"555-1234\"),\n",
        "    Row(id=3, first_name=\"John\", last_name=\"Smyth\", email=\"john.smyth@email.com\", phone=\"555-1235\"),\n",
        "    Row(id=4, first_name=\"Jane\", last_name=\"Doe\", email=\"jane.doe@email.com\", phone=\"555-5678\"),\n",
        "    Row(id=5, first_name=\"Jane\", last_name=\"Doe\", email=\"jane.doe@email.com\", phone=\"555-5678\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(sample_data)\n",
        "print(\"âœ… Sample dataset created:\")\n",
        "df.show()\n",
        "\n",
        "print(f\"Dataset has {df.count()} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Splink with Spark backend\n",
        "from splink import Linker\n",
        "from splink.backends.spark import SparkAPI\n",
        "from splink.comparison_library import ExactMatch, LevenshteinAtThresholds\n",
        "\n",
        "print(\"Configuring Splink...\")\n",
        "\n",
        "settings = {\n",
        "    \"link_type\": \"dedupe_only\",\n",
        "    \"blocking_rules_to_generate_predictions\": [\n",
        "        \"l.first_name = r.first_name\",\n",
        "        \"l.last_name = r.last_name\",\n",
        "    ],\n",
        "    \"comparisons\": [\n",
        "        ExactMatch(\"first_name\"),\n",
        "        ExactMatch(\"last_name\"),\n",
        "        ExactMatch(\"email\"),\n",
        "        ExactMatch(\"phone\"),\n",
        "        LevenshteinAtThresholds(\"first_name\", 2),\n",
        "        LevenshteinAtThresholds(\"last_name\", 2),\n",
        "    ],\n",
        "    \"retain_matching_columns\": True,\n",
        "    \"retain_intermediate_calculation_columns\": True,\n",
        "}\n",
        "\n",
        "# Initialize Splink linker with Spark backend\n",
        "spark_api = SparkAPI(spark)\n",
        "linker = Linker(df, settings, spark_api)\n",
        "print(\"âœ… Splink linker initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Training Splink model...\")\n",
        "\n",
        "try:\n",
        "    linker.estimate_probability_two_random_records_match(\n",
        "        [\"l.first_name = r.first_name\", \"l.last_name = r.last_name\"],\n",
        "        recall=0.7\n",
        "    )\n",
        "    print(\"âœ… Probability estimation completed\")\n",
        "\n",
        "    linker.estimate_u_using_random_sampling(max_pairs=1e6)\n",
        "    print(\"âœ… U estimation completed\")\n",
        "\n",
        "    linker.estimate_parameters_using_expectation_maximisation(\"l.first_name = r.first_name\")\n",
        "    print(\"âœ… Parameter estimation completed\")\n",
        "\n",
        "    print(\"âœ… Model training completed successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training error: {e}\")\n",
        "    print(\"This might be due to insufficient data or configuration issues\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "print(\"Getting predictions...\")\n",
        "\n",
        "try:\n",
        "    predictions = linker.predict()\n",
        "    print(\"âœ… Predictions generated:\")\n",
        "    predictions.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Prediction error: {e}\")\n",
        "    print(\"This might be due to model training issues\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get clusters\n",
        "print(\"Getting clusters...\")\n",
        "\n",
        "try:\n",
        "    clusters = linker.cluster_pairwise_predictions_at_threshold(predictions, threshold_match_probability=0.5)\n",
        "    print(\"âœ… Clusters generated:\")\n",
        "    clusters.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Clustering error: {e}\")\n",
        "    print(\"This might be due to prediction issues\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary and next steps\n",
        "print(\"ðŸŽ‰ Splink with Spark backend demo completed!\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY:\")\n",
        "print(\"=\"*50)\n",
        "print(\"âœ… All imports working\")\n",
        "print(\"âœ… Spark session initialized with Iceberg support\")\n",
        "print(\"âœ… Sample data created\")\n",
        "print(\"âœ… Splink linker configured\")\n",
        "print(\"âœ… Model training attempted\")\n",
        "print(\"âœ… Predictions generated\")\n",
        "print(\"âœ… Clusters created\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"NEXT STEPS:\")\n",
        "print(\"=\"*50)\n",
        "print(\"1. Replace sample data with your actual Iceberg table:\")\n",
        "print(\"   df = spark.sql('SELECT * FROM nessie.your_database.your_table')\")\n",
        "print(\"\\n2. Adjust blocking rules and comparisons based on your data schema\")\n",
        "print(\"\\n3. Save results back to Iceberg:\")\n",
        "print(\"   clusters.writeTo('nessie.your_database.results_table').createOrReplace()\")\n",
        "print(\"\\n4. For production use, consider:\")\n",
        "print(\"   - Larger datasets for better model training\")\n",
        "print(\"   - More sophisticated blocking rules\")\n",
        "print(\"   - Additional comparison functions\")\n",
        "print(\"   - Performance tuning for large-scale data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splink with Spark Backend - Iceberg Tables Demo\n",
        "\n",
        "This notebook demonstrates how to use Splink with Spark backend to read data from Iceberg tables stored in MinIO via Nessie catalog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Required packages are pre-installed in the container\n",
        "# If you need to install additional packages, uncomment the line below:\n",
        "# !pip install package_name\n",
        "\n",
        "# Verify that Splink is available\n",
        "try:\n",
        "    from splink.spark.linker import SparkLinker\n",
        "    print(\"âœ… Splink is available!\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Splink import failed: {e}\")\n",
        "    print(\"Installing Splink...\")\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', 'splink[spark]'], check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'splink.comparison_template_library'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msplink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkAPI\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msplink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomparison_library\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExactMatch, LevenshteinAtThresholds\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msplink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomparison_template_library\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m name_comparison\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set AWS environment variables for MinIO access\u001b[39;00m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAWS_ACCESS_KEY_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminio\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'splink.comparison_template_library'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from splink import Linker\n",
        "from splink.backends.spark import SparkAPI\n",
        "from splink.comparison_library import ExactMatch, LevenshteinAtThresholds\n",
        "from splink.comparison_template_library import name_comparison\n",
        "\n",
        "# Set AWS environment variables for MinIO access\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio12345'\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
        "os.environ['AWS_ENDPOINT_URL'] = 'http://minio:9000'\n",
        "os.environ['AWS_ENDPOINT_URL_S3'] = 'http://minio:9000'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark session with Iceberg support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SplinkIcebergDemo\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.type\", \"nessie\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.ref\", \"main\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://warehouse/\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio12345\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark session initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available catalogs and tables\n",
        "print(\"Available catalogs:\")\n",
        "spark.sql(\"SHOW CATALOGS\").show()\n",
        "\n",
        "print(\"\\nAvailable databases in nessie catalog:\")\n",
        "spark.sql(\"SHOW DATABASES IN nessie\").show()\n",
        "\n",
        "print(\"\\nAvailable tables in nessie catalog:\")\n",
        "spark.sql(\"SHOW TABLES IN nessie\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Read data from an Iceberg table (replace 'your_table' with actual table name)\n",
        "# df = spark.sql(\"SELECT * FROM nessie.your_database.your_table LIMIT 10\")\n",
        "# df.show()\n",
        "\n",
        "# For demonstration, let's create a sample dataset\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Sample data for testing Splink\n",
        "sample_data = [\n",
        "    Row(id=1, first_name=\"John\", last_name=\"Smith\", email=\"john.smith@email.com\", phone=\"555-1234\"),\n",
        "    Row(id=2, first_name=\"Jon\", last_name=\"Smith\", email=\"jon.smith@email.com\", phone=\"555-1234\"),\n",
        "    Row(id=3, first_name=\"John\", last_name=\"Smyth\", email=\"john.smyth@email.com\", phone=\"555-1235\"),\n",
        "    Row(id=4, first_name=\"Jane\", last_name=\"Doe\", email=\"jane.doe@email.com\", phone=\"555-5678\"),\n",
        "    Row(id=5, first_name=\"Jane\", last_name=\"Doe\", email=\"jane.doe@email.com\", phone=\"555-5678\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(sample_data)\n",
        "print(\"Sample dataset:\")\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Splink with Spark backend\n",
        "settings = {\n",
        "    \"link_type\": \"dedupe_only\",\n",
        "    \"blocking_rules_to_generate_predictions\": [\n",
        "        \"l.first_name = r.first_name\",\n",
        "        \"l.last_name = r.last_name\",\n",
        "    ],\n",
        "    \"comparisons\": [\n",
        "        ExactMatch(\"first_name\"),\n",
        "        ExactMatch(\"last_name\"),\n",
        "        ExactMatch(\"email\"),\n",
        "        ExactMatch(\"phone\"),\n",
        "        LevenshteinAtThresholds(\"first_name\", 2),\n",
        "        LevenshteinAtThresholds(\"last_name\", 2),\n",
        "    ],\n",
        "    \"retain_matching_columns\": True,\n",
        "    \"retain_intermediate_calculation_columns\": True,\n",
        "}\n",
        "\n",
        "# Initialize Splink linker with Spark backend\n",
        "spark_api = SparkAPI(spark)\n",
        "linker = Linker(df, settings, spark_api)\n",
        "print(\"Splink linker initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "linker.estimate_probability_two_random_records_match(\n",
        "    [\"l.first_name = r.first_name\", \"l.last_name = r.last_name\"],\n",
        "    recall=0.7\n",
        ")\n",
        "\n",
        "linker.estimate_u_using_random_sampling(max_pairs=1e6)\n",
        "linker.estimate_parameters_using_expectation_maximisation(\"l.first_name = r.first_name\")\n",
        "\n",
        "print(\"Model training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "predictions = linker.predict()\n",
        "print(\"Predictions:\")\n",
        "predictions.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get clusters\n",
        "clusters = linker.cluster_pairwise_predictions_at_threshold(predictions, threshold_match_probability=0.5)\n",
        "print(\"Clusters:\")\n",
        "clusters.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Save results back to Iceberg table\n",
        "# clusters.writeTo(\"nessie.your_database.clustered_results\").createOrReplace()\n",
        "\n",
        "print(\"Splink with Spark backend demo completed successfully!\")\n",
        "print(\"\\nTo use with your actual Iceberg tables:\")\n",
        "print(\"1. Replace the sample data with: df = spark.sql('SELECT * FROM nessie.your_database.your_table')\")\n",
        "print(\"2. Adjust the blocking rules and comparisons based on your data schema\")\n",
        "print(\"3. Save results back to Iceberg using: results.writeTo('nessie.your_database.results_table').createOrReplace()\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
