{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splink with Spark Backend - Iceberg Tables Demo\n",
        "\n",
        "This notebook demonstrates how to use Splink with Spark backend to read data from Iceberg tables stored in MinIO via Nessie catalog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install splink[spark] pyiceberg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from splink.spark.linker import SparkLinker\n",
        "from splink.spark.comparison_library import exact_match, levenshtein_at_thresholds\n",
        "from splink.spark.comparison_template_library import name_comparison\n",
        "\n",
        "# Set AWS environment variables for MinIO access\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio12345'\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
        "os.environ['AWS_ENDPOINT_URL'] = 'http://minio:9000'\n",
        "os.environ['AWS_ENDPOINT_URL_S3'] = 'http://minio:9000'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark session with Iceberg support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SplinkIcebergDemo\") \\\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.type\", \"nessie\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.ref\", \"main\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://warehouse/\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\") \\\n",
        "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio12345\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark session initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available catalogs and tables\n",
        "print(\"Available catalogs:\")\n",
        "spark.sql(\"SHOW CATALOGS\").show()\n",
        "\n",
        "print(\"\\nAvailable databases in nessie catalog:\")\n",
        "spark.sql(\"SHOW DATABASES IN nessie\").show()\n",
        "\n",
        "print(\"\\nAvailable tables in nessie catalog:\")\n",
        "spark.sql(\"SHOW TABLES IN nessie\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Read data from an Iceberg table (replace 'your_table' with actual table name)\n",
        "# df = spark.sql(\"SELECT * FROM nessie.your_database.your_table LIMIT 10\")\n",
        "# df.show()\n",
        "\n",
        "# For demonstration, let's create a sample dataset\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Sample data for testing Splink\n",
        "sample_data = [\n",
        "    Row(id=1, first_name=\"John\", last_name=\"Smith\", email=\"john.smith@email.com\", phone=\"555-1234\"),\n",
        "    Row(id=2, first_name=\"Jon\", last_name=\"Smith\", email=\"jon.smith@email.com\", phone=\"555-1234\"),\n",
        "    Row(id=3, first_name=\"John\", last_name=\"Smyth\", email=\"john.smyth@email.com\", phone=\"555-1235\"),\n",
        "    Row(id=4, first_name=\"Jane\", last_name=\"Doe\", email=\"jane.doe@email.com\", phone=\"555-5678\"),\n",
        "    Row(id=5, first_name=\"Jane\", last_name=\"Doe\", email=\"jane.doe@email.com\", phone=\"555-5678\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(sample_data)\n",
        "print(\"Sample dataset:\")\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Splink with Spark backend\n",
        "settings = {\n",
        "    \"link_type\": \"dedupe_only\",\n",
        "    \"blocking_rules_to_generate_predictions\": [\n",
        "        \"l.first_name = r.first_name\",\n",
        "        \"l.last_name = r.last_name\",\n",
        "    ],\n",
        "    \"comparisons\": [\n",
        "        exact_match(\"first_name\"),\n",
        "        exact_match(\"last_name\"),\n",
        "        exact_match(\"email\"),\n",
        "        exact_match(\"phone\"),\n",
        "        levenshtein_at_thresholds(\"first_name\", 2),\n",
        "        levenshtein_at_thresholds(\"last_name\", 2),\n",
        "    ],\n",
        "    \"retain_matching_columns\": True,\n",
        "    \"retain_intermediate_calculation_columns\": True,\n",
        "}\n",
        "\n",
        "# Initialize Splink linker\n",
        "linker = SparkLinker(df, settings)\n",
        "print(\"Splink linker initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "linker.estimate_probability_two_random_records_match(\n",
        "    [\"l.first_name = r.first_name\", \"l.last_name = r.last_name\"],\n",
        "    recall=0.7\n",
        ")\n",
        "\n",
        "linker.estimate_u_using_random_sampling(max_pairs=1e6)\n",
        "linker.estimate_parameters_using_expectation_maximisation(\"l.first_name = r.first_name\")\n",
        "\n",
        "print(\"Model training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "predictions = linker.predict()\n",
        "print(\"Predictions:\")\n",
        "predictions.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get clusters\n",
        "clusters = linker.cluster_pairwise_predictions_at_threshold(predictions, threshold_match_probability=0.5)\n",
        "print(\"Clusters:\")\n",
        "clusters.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Save results back to Iceberg table\n",
        "# clusters.writeTo(\"nessie.your_database.clustered_results\").createOrReplace()\n",
        "\n",
        "print(\"Splink with Spark backend demo completed successfully!\")\n",
        "print(\"\\nTo use with your actual Iceberg tables:\")\n",
        "print(\"1. Replace the sample data with: df = spark.sql('SELECT * FROM nessie.your_database.your_table')\")\n",
        "print(\"2. Adjust the blocking rules and comparisons based on your data schema\")\n",
        "print(\"3. Save results back to Iceberg using: results.writeTo('nessie.your_database.results_table').createOrReplace()\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
