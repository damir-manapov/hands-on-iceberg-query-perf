/**
 * generate.ts ‚Äî Generate, load & measure Iceberg/Parquet table
 */

import { SizeRow, TableConfig } from "../types";
import { TrinoClient } from "../TrinoClient";
import {
  buildInsertSQL,
  createFirstRowSQL,
  createLastRowSQL,
  createSchemaSQL,
  createTableSQL,
} from "../sqlHelpers";
import { humanNumber, humanSize, makeBatches } from "../utils";
import { Limiter } from "../Limiter";
import { TABLE_CONFIGS } from "../config/tableConfigs";
import { LOAD } from "../config/load";
import { createTrinoConfig } from "../config/trinoConfig";
import { CONNECTIONS } from "../config/tableQueries";
// Fixed codec setup: always use zstd with level 6
const FIXED_CODEC = "zstd" as const;
const FIXED_LEVEL = 6 as const;

function formatETA(ms: number): string {
  if (ms < 1000) return `${Math.round(ms)}ms`;
  if (ms < 60000) return `${(ms / 1000).toFixed(1)}s`;
  if (ms < 3600000) return `${(ms / 60000).toFixed(1)}min`;
  return `${(ms / 3600000).toFixed(1)}h`;
}

function calculateBatchETA(
  completedBatches: number,
  totalBatches: number,
  startTime: number,
  currentTime: number
): string {
  if (completedBatches === 0) return "calculating...";

  const elapsed = currentTime - startTime;
  const rate = completedBatches / elapsed; // batches per ms
  const remaining = totalBatches - completedBatches;
  const etaMs = remaining / rate;

  return formatETA(etaMs);
}

async function loadTable(
  client: TrinoClient,
  limiter: Limiter,
  cfg: TableConfig,
  tableName: string,
  startId: number,
  totalRows: number,
  batchRows: number
) {
  // Check current row count in the table
  const fullTableName = `${cfg.catalog}.${cfg.schema}.${tableName}`;
  const countSQL = `SELECT COUNT(*) as count FROM ${fullTableName}`;

  let currentRowCount = 0;
  try {
    const countResult = await client.query(countSQL);
    currentRowCount = (countResult[0] as { count: number })?.count ?? 0;
    console.log(
      `üìä Current row count in ${tableName}: ${currentRowCount.toLocaleString()}`
    );
  } catch {
    console.log(
      `üìä Table ${tableName} doesn't exist yet, starting from 0 rows`
    );
    currentRowCount = 0;
  }

  // Calculate how many more rows we need to generate
  const remainingRows = totalRows - currentRowCount;

  if (remainingRows <= 0) {
    console.log(
      `‚úì Table ${tableName} already has ${currentRowCount.toLocaleString()} rows (target: ${totalRows.toLocaleString()})`
    );
    return;
  }

  console.log(
    `üìà Need to generate ${remainingRows.toLocaleString()} more rows (${currentRowCount.toLocaleString()} + ${remainingRows.toLocaleString()} = ${totalRows.toLocaleString()})`
  );

  // Calculate the starting ID for new rows
  const nextId = startId + currentRowCount;

  // Create batches for the remaining rows
  const batches = makeBatches(nextId, remainingRows, batchRows);

  const startTime = Date.now();
  const totalBatches = batches.length;

  console.log(
    `\nüìä Loading ${tableName}: ${totalBatches} batches for remaining ${remainingRows.toLocaleString()} rows`
  );

  await Promise.all(
    batches.map(b =>
      limiter.run(async () => {
        const label = `${tableName} #${b.index}/${batches.length} [${humanNumber(b.start)}..${humanNumber(b.end)}]`;
        const sql = buildInsertSQL(b.start, b.end, cfg, tableName, totalRows);
        const t0 = Date.now();
        try {
          // Calculate ETA before starting the batch
          const eta = calculateBatchETA(
            b.index - 1,
            totalBatches,
            startTime,
            Date.now()
          );
          console.log(`‚Üí INSERT ${label} (ETA: ${eta})`);
          await client.execute(sql);
          const duration = ((Date.now() - t0) / 1000).toFixed(1);
          const progress = ((b.index / totalBatches) * 100).toFixed(1);
          console.log(`‚úî OK ${label} in ${duration}s (${progress}% complete)`);
        } catch (e: unknown) {
          const err = e as { message?: string; stack?: string } | string;
          const errorMessage =
            typeof err === "string" ? err : err?.message || String(err);
          const stack =
            typeof err === "object" && err?.stack ? err.stack : undefined;

          console.error(`\nüö® ERROR in ${label}:`);
          console.error(`   Message: ${errorMessage}`);
          if (stack) {
            console.error(`   Stack trace:`);
            console.error(stack);
          }
          console.error(`   SQL: ${sql}`);
          console.error(`\n`);

          throw new Error(`FAIL ${label}: ${errorMessage}`);
        }
      })
    )
  );
}

async function optimizeTable(client: TrinoClient, fq: string) {
  const optimizeSql = `ALTER TABLE ${fq} EXECUTE optimize`;
  const expireSnapshotsSql = `ALTER TABLE ${fq} EXECUTE expire_snapshots(retention_threshold => '1s')`;
  const removeOrphanFilesSql = `ALTER TABLE ${fq} EXECUTE remove_orphan_files(retention_threshold => '1s')`;

  try {
    console.log(`üîß Optimizing table ${fq}...`);
    await client.execute(optimizeSql);
    console.log(`‚úÖ Table ${fq} optimized successfully`);

    // Create a new client with session properties for cleanup
    console.log(`‚öôÔ∏è Creating cleanup client with 1s retention...`);
    const cleanupClient = new TrinoClient({
      ...client.config,
      sessionProperties: {
        "iceberg.expire_snapshots_min_retention": "1s",
        "iceberg.remove_orphan_files_min_retention": "1s",
      },
    });

    console.log(`üóëÔ∏è Expiring old snapshots for ${fq}...`);
    await cleanupClient.execute(expireSnapshotsSql);
    console.log(`‚úÖ Old snapshots expired for ${fq}`);

    console.log(`üßπ Removing orphan files for ${fq}...`);
    await cleanupClient.execute(removeOrphanFilesSql);
    console.log(`‚úÖ Orphan files removed for ${fq}`);
  } catch (e: unknown) {
    const err = e as { message?: string; stack?: string } | string;
    const errorMessage =
      typeof err === "string" ? err : err?.message || String(err);
    const stack = typeof err === "object" && err?.stack ? err.stack : undefined;

    console.error(`\nüö® OPTIMIZE/MAINTENANCE ERROR for ${fq}:`);
    console.error(`   Message: ${errorMessage}`);
    if (stack) {
      console.error(`   Stack trace:`);
      console.error(stack);
    }
    console.error(`   SQL: ${optimizeSql}`);
    console.error(`   SQL: ${expireSnapshotsSql}`);
    console.error(`   SQL: ${removeOrphanFilesSql}`);
    console.error(`\n`);

    throw new Error(`Optimize/Maintenance failed for ${fq}: ${errorMessage}`);
  }
}

async function measureSizes(
  client: TrinoClient,
  cfg: TableConfig,
  name: string,
  connectionId: string,
  connectionName: string
): Promise<SizeRow> {
  const filesTbl = `"${name}$files"`;
  const baseSQL = `SELECT SUM(file_size_in_bytes) AS data_bytes, SUM(record_count) AS rows, (SUM(file_size_in_bytes) / NULLIF(SUM(record_count),0)) AS bytes_per_row FROM ${cfg.catalog}.${cfg.schema}.${filesTbl}`;
  const filesRes = await client.query<{
    data_bytes: number;
    rows: number;
    bytes_per_row: number;
  }>(baseSQL);
  const data_bytes = Number(filesRes?.[0]?.data_bytes ?? 0);
  const rows = Number(filesRes?.[0]?.rows ?? 0);
  const bytes_per_row = Number(filesRes?.[0]?.bytes_per_row ?? 0);

  if (!LOAD.includeManifestBytes) {
    return {
      table_name: name,
      codec: FIXED_CODEC,
      level: FIXED_LEVEL,
      rows,
      data_bytes,
      bytes_per_row,
      connection_id: connectionId,
      connection_name: connectionName,
    };
  }

  // manifests (optional)
  let manifest_bytes = 0;
  try {
    const m = await client.query<{ b: number }>(`
      SELECT COALESCE(SUM(length),0) AS b
      FROM ${cfg.catalog}.${cfg.schema}."${name}$manifests"
    `);
    manifest_bytes = Number(m?.[0]?.b ?? 0);
  } catch (e: unknown) {
    throw new Error(String(e));
  }
  const total_bytes = data_bytes + manifest_bytes;
  return {
    table_name: name,
    codec: FIXED_CODEC,
    level: FIXED_LEVEL,
    rows,
    data_bytes,
    bytes_per_row,
    manifest_bytes,
    total_bytes,
    connection_id: connectionId,
    connection_name: connectionName,
  };
}

async function main() {
  const allResults: SizeRow[] = [];

  // Process each connection
  for (const [connectionId, connection] of Object.entries(CONNECTIONS)) {
    console.log(
      `\nüîó Processing connection: ${connection.description} (${connection.host}:${connection.port})`
    );

    // Trino connection for this specific connection
    const trino = createTrinoConfig("load", connectionId);
    const client = new TrinoClient(trino);
    const limiter = new Limiter(LOAD.concurrency);

    const connectionResults: SizeRow[] = [];

    // Process each table configuration
    for (const tableConfig of TABLE_CONFIGS) {
      if (tableConfig.enabled === false) {
        console.log(`‚è≠Ô∏è  Skipping disabled table: ${tableConfig.tableBase}`);
        continue;
      }
      // Process each row count for this table
      for (const totalRows of tableConfig.totalRows) {
        const batchRows = tableConfig.batchRows;

        // Generate dynamic table name with row count
        const rowCountSuffix = humanNumber(totalRows).replace(/,/g, "");
        const name = `${tableConfig.tableBase}_${rowCountSuffix}`;

        console.log(
          `\nüöÄ Processing table: ${name} (${humanNumber(totalRows)} rows, batch size: ${humanNumber(batchRows)})`
        );

        try {
          // 1) Base schema
          if (LOAD.createBaseSchema) {
            console.log(`Ensuring schema exists for ${name}‚Ä¶`);
            await client.execute(createSchemaSQL(tableConfig));
          }
          console.log(`Creating table ${name}‚Ä¶`);
          await client.execute(createTableSQL(tableConfig, name));

          const t0 = Date.now();
          await loadTable(
            client,
            limiter,
            tableConfig,
            name,
            LOAD.startId,
            totalRows,
            batchRows
          );
          const durationMs = Date.now() - t0;
          const durationMinutes = (durationMs / 60000).toFixed(1);
          console.log(`Load finished for ${name} in ${durationMinutes}min`);

          if (LOAD.compactAfterLoad) {
            await optimizeTable(
              client,
              `${tableConfig.catalog}.${tableConfig.schema}.${name}`
            );
          }

          // Show first and last rows ordered by ID
          const firstRow = await client.query(
            createFirstRowSQL(tableConfig, name)
          );
          console.log(
            `\nFirst row (ordered by ${tableConfig.idColumn}) for ${name}:`
          );
          console.log(JSON.stringify(firstRow, null, 1));

          const lastRow = await client.query(
            createLastRowSQL(tableConfig, name)
          );
          console.log(
            `\nLast row (ordered by ${tableConfig.idColumn}) for ${name}:`
          );
          console.log(JSON.stringify(lastRow, null, 1));

          // Measure immediately and collect result
          const result = await measureSizes(
            client,
            tableConfig,
            name,
            connectionId,
            connection.name
          );
          connectionResults.push(result);
        } catch (e: unknown) {
          const err = e as { message?: string; stack?: string } | string;
          const errorMessage =
            typeof err === "string" ? err : err?.message || String(err);
          const stack =
            typeof err === "object" && err?.stack ? err.stack : undefined;

          console.error(`\nüö® TABLE PROCESSING ERROR for ${name}:`);
          console.error(`   Message: ${errorMessage}`);
          if (stack) {
            console.error(`   Stack trace:`);
            console.error(stack);
          }
          console.error(`\n`);

          throw new Error(
            `Table processing failed for ${name}: ${errorMessage}`
          );
        }
      }
    }

    // Add results to overall results
    allResults.push(...connectionResults);

    console.log(
      `\n‚úÖ Completed data generation for connection: ${connection.description}`
    );
  }

  // 3) Print results & CSV
  console.log("\nüìä RESULTS (bytes rounded):");
  const prettyResults = allResults.map(result => ({
    connection: result.connection_name,
    table: result.table_name,
    codec: result.codec,
    level: result.level,
    rows: humanNumber(result.rows),
    data_bytes: Math.round(result.data_bytes),
    data_human: humanSize(result.data_bytes),
    bytes_per_row: Math.round(result.bytes_per_row),
    ...(LOAD.includeManifestBytes
      ? {
          manifest_bytes: Math.round(result.manifest_bytes || 0),
          manifest_human: humanSize(result.manifest_bytes || 0),
          total_bytes: Math.round(result.total_bytes || result.data_bytes),
          total_human: humanSize(result.total_bytes || result.data_bytes),
        }
      : {}),
  }));
  console.table(prettyResults);
}

main().catch(e => {
  console.error(`\nüö® FATAL ERROR:`);
  console.error(`   ${e}`);
  if (e instanceof Error && e.stack) {
    console.error(`\n   Stack trace:`);
    console.error(e.stack);
  }
  console.error(`\n   Process exiting with code 1\n`);
  process.exit(1);
});
